{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sub import subMNIST       # testing the subclass of MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset_import = pickle.load(open(\"train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "semiset_import = pickle.load(open('train_unlabeled.p', \"rb\"))\n",
    "train_loader = torch.utils.data.DataLoader(trainset_import, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)\n",
    "semi_loader = torch.utils.data.DataLoader(semiset_import, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semiset_import.train_labels = torch.LongTensor(len(semi_loader.dataset)).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "â‹® \n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 47000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semiset_import.train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_transformer(loader, rotation=True):\n",
    "    result = []\n",
    "    for data, target in loader:\n",
    "        size = data.size(0)\n",
    "        data = data.numpy().reshape(size, 784)\n",
    "        if rotation:\n",
    "            data_3D = data.reshape(size, 28, 28) #64*28*28\n",
    "            for i in range(9):\n",
    "                data_new = np.empty([0,28,28])\n",
    "                for graph in data_3D:\n",
    "                    b = scipy.misc.imrotate(graph, random.randint(-15, 15)).reshape(1,28,28)\n",
    "                    data_new = np.concatenate((data_new, b), axis=0)    \n",
    "                max_val = np.max(data_new)\n",
    "                min_val = np.min(data_new)\n",
    "                data_new_norm = (data_new-min_val) / (max_val-min_val) #64*784\n",
    "                Y = torch.from_numpy(data_new_norm.reshape(size, 784))\n",
    "                Y = torch.FloatTensor(size,784).copy_(Y)\n",
    "                #target = torch.FloatTensor(size).copy_(target)\n",
    "                pair = (Y, target)\n",
    "                result.append(pair)\n",
    "        \n",
    "        max_val = np.max(data)\n",
    "        min_val = np.min(data)\n",
    "        data_norm = (data-min_val) / (max_val-min_val) #64*784\n",
    "            \n",
    "        X = torch.from_numpy(data_norm)\n",
    "        pair = (X, target)\n",
    "        result.append(pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "train_trans = data_transformer(train_loader, rotation=True)\n",
    "valid_trans = data_transformer(valid_loader, rotation=False)\n",
    "semi_trans = data_transformer(semi_loader, rotation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import argparse\n",
    "#from torchvision import datasets, transforms\n",
    "#import torch.autograd as autograd\n",
    "#from __future__ import print_function\n",
    "\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "h_dim = 128\n",
    "y_dim = 10\n",
    "lr = 1e-3\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_y = xavier_init(size=[h_dim, y_dim])\n",
    "bhz_y = Variable(torch.zeros(y_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h_initial = F.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    dropout = torch.nn.Dropout(p = 0.0)\n",
    "    h = dropout(h_initial)\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    recon_y = h @ Whz_y + bhz_y.repeat(h.size(0), 1)\n",
    "    prop_y = F.log_softmax(recon_y)\n",
    "    return z_mu, z_var, prop_y\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    size = mu.size(0)\n",
    "    eps = Variable(torch.randn(size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "# Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "# bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "# Whx = xavier_init(size=[h_dim, X_dim])\n",
    "# bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "# def P(z):\n",
    "#     h = F.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "#     X = F.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "#     return X\n",
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = F.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = F.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var, Whz_y, bhz_y,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "Train_loss, Train_label_loss, Test_loss, Test_label_loss = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "def train(epoch, train_trans = train_trans, lamba=0):\n",
    "    train_loss = 0\n",
    "    label_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in train_trans:\n",
    "        X, target = Variable(data), Variable(target)\n",
    "        size = len(target)\n",
    "        z_mu, z_var, recon_y = Q(X)\n",
    "        z = sample_z(z_mu, z_var)\n",
    "        X_sample = P(z, recon_y)\n",
    "#         X_sample = P(z)\n",
    "\n",
    "    # Loss\n",
    "        recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False) /size\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "#         print(target)\n",
    "        predict_loss = F.nll_loss(recon_y, target)\n",
    "        loss = recon_loss + kl_loss + lamba* predict_loss\n",
    "        train_loss += recon_loss.data[0]\n",
    "        label_loss += predict_loss.data[0]\n",
    "        pred = recon_y.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    # Backward\n",
    "        loss.backward()\n",
    "\n",
    "    # Update\n",
    "        solver.step()\n",
    "#         print(pred)\n",
    "\n",
    "    # Housekeeping\n",
    "        for p in params:\n",
    "            p.grad.data.zero_()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    train_loss /= len(train_trans)  # loss function already averages over batch size\n",
    "    label_loss /= len(train_trans) \n",
    "    Train_loss.append(train_loss)\n",
    "    Train_label_loss.append(label_loss)\n",
    "    print('Train set: Train loss: {:.4f}, label loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    train_loss, label_loss, correct, len(train_trans)*64,\n",
    "    100. * correct / (len(train_trans)*64)))\n",
    "\n",
    "    samples = P(z, recon_y).data.numpy()[:16]\n",
    "#     samples = P(z).data.numpy()[:16]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    if not os.path.exists('out/'):\n",
    "        os.makedirs('out/')\n",
    "\n",
    "    plt.savefig('out/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def test(epoch):\n",
    "    test_loss = 0\n",
    "    label_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_trans:\n",
    "        X, target = Variable(data), Variable(target)\n",
    "        z_mu, z_var, recon_y = Q(X)\n",
    "        z = sample_z(z_mu, z_var)\n",
    "        X_sample = P(z, recon_y)\n",
    "#         X_sample = P(z)\n",
    "\n",
    "    # Loss\n",
    "        recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "        predict_loss = F.nll_loss(recon_y, target)\n",
    "        loss = recon_loss + kl_loss + predict_loss\n",
    "        test_loss += recon_loss.data[0]\n",
    "        label_loss += predict_loss.data[0]\n",
    "        pred = recon_y.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    label_loss /= len(valid_loader)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_label_loss.append(label_loss)\n",
    "    print('Valid set: Valid loss: {:.4f}, label loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, label_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "\n",
    "def semi():    \n",
    "    for index, (data, target) in enumerate(semi_trans):\n",
    "        size = len(target)\n",
    "        X, target = Variable(data), Variable(target)\n",
    "        z_mu, z_var, recon_y = Q(X)\n",
    "        z = sample_z(z_mu, z_var)\n",
    "        X_sample = P(z, recon_y)\n",
    "#         X_sample = P(z)\n",
    "        pred = recon_y.data.max(1)[1] # get the index of the max log-probability\n",
    "        new = torch.LongTensor(size).copy_(pred)\n",
    "#         print(new)\n",
    "        semi_trans[index][1].copy_(new)\n",
    "#     print(semi_trans[-1][1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Train set: Train loss: 199.7100, label loss: 0.4429, Accuracy: 26228/30080 (87%)\n",
      "Valid set: Valid loss: 144.8090, label loss: 0.3421, Accuracy: 8922/10000 (89%)\n",
      "epoch: 1\n",
      "Train set: Train loss: 141.8414, label loss: 0.1822, Accuracy: 28453/30080 (95%)\n",
      "Valid set: Valid loss: 130.2178, label loss: 0.2738, Accuracy: 9194/10000 (92%)\n",
      "epoch: 2\n",
      "Train set: Train loss: 132.6433, label loss: 0.1028, Accuracy: 29199/30080 (97%)\n",
      "Valid set: Valid loss: 123.9047, label loss: 0.2454, Accuracy: 9303/10000 (93%)\n",
      "epoch: 3\n",
      "Train set: Train loss: 127.7139, label loss: 0.0636, Accuracy: 29532/30080 (98%)\n",
      "Valid set: Valid loss: 119.6071, label loss: 0.2426, Accuracy: 9333/10000 (93%)\n",
      "epoch: 4\n",
      "Train set: Train loss: 124.3730, label loss: 0.0416, Accuracy: 29728/30080 (99%)\n",
      "Valid set: Valid loss: 116.7172, label loss: 0.2491, Accuracy: 9350/10000 (94%)\n",
      "epoch: 5\n",
      "Train set: Train loss: 121.8324, label loss: 0.0272, Accuracy: 29842/30080 (99%)\n",
      "Valid set: Valid loss: 114.5529, label loss: 0.2627, Accuracy: 9352/10000 (94%)\n",
      "epoch: 6\n",
      "Train set: Train loss: 119.7620, label loss: 0.0180, Accuracy: 29907/30080 (99%)\n",
      "Valid set: Valid loss: 112.8396, label loss: 0.2874, Accuracy: 9325/10000 (93%)\n",
      "epoch: 7\n",
      "Train set: Train loss: 117.8262, label loss: 0.0119, Accuracy: 29960/30080 (100%)\n",
      "Valid set: Valid loss: 111.0518, label loss: 0.3163, Accuracy: 9269/10000 (93%)\n",
      "epoch: 8\n",
      "Train set: Train loss: 116.3151, label loss: 0.0080, Accuracy: 29975/30080 (100%)\n",
      "Valid set: Valid loss: 109.7758, label loss: 0.3096, Accuracy: 9307/10000 (93%)\n",
      "epoch: 9\n",
      "Train set: Train loss: 114.8660, label loss: 0.0054, Accuracy: 29993/30080 (100%)\n",
      "Valid set: Valid loss: 108.2280, label loss: 0.2960, Accuracy: 9360/10000 (94%)\n",
      "epoch: 10\n",
      "Train set: Train loss: 113.6437, label loss: 0.0037, Accuracy: 29998/30080 (100%)\n",
      "Valid set: Valid loss: 107.3866, label loss: 0.2888, Accuracy: 9395/10000 (94%)\n",
      "epoch: 11\n",
      "Train set: Train loss: 112.5820, label loss: 0.0025, Accuracy: 29998/30080 (100%)\n",
      "Valid set: Valid loss: 106.5848, label loss: 0.2950, Accuracy: 9402/10000 (94%)\n",
      "epoch: 12\n",
      "Train set: Train loss: 111.5699, label loss: 0.0018, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 105.1528, label loss: 0.3138, Accuracy: 9374/10000 (94%)\n",
      "epoch: 13\n",
      "Train set: Train loss: 110.6422, label loss: 0.0013, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 105.1536, label loss: 0.3323, Accuracy: 9358/10000 (94%)\n",
      "epoch: 14\n",
      "Train set: Train loss: 109.7961, label loss: 0.0010, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 104.0937, label loss: 0.3348, Accuracy: 9371/10000 (94%)\n",
      "epoch: 15\n",
      "Train set: Train loss: 108.9951, label loss: 0.0007, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 104.0388, label loss: 0.3303, Accuracy: 9397/10000 (94%)\n",
      "epoch: 16\n",
      "Train set: Train loss: 108.3226, label loss: 0.0006, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 103.6444, label loss: 0.3316, Accuracy: 9413/10000 (94%)\n",
      "epoch: 17\n",
      "Train set: Train loss: 107.6623, label loss: 0.0005, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 102.6168, label loss: 0.3302, Accuracy: 9425/10000 (94%)\n",
      "epoch: 18\n",
      "Train set: Train loss: 107.0350, label loss: 0.0004, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 102.5223, label loss: 0.3309, Accuracy: 9443/10000 (94%)\n",
      "epoch: 19\n",
      "Train set: Train loss: 106.4379, label loss: 0.0003, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 101.2206, label loss: 0.3376, Accuracy: 9438/10000 (94%)\n",
      "epoch: 20\n",
      "Train set: Train loss: 105.9385, label loss: 0.0003, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 100.8292, label loss: 0.3407, Accuracy: 9446/10000 (94%)\n",
      "epoch: 21\n",
      "Train set: Train loss: 105.3854, label loss: 0.0002, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 99.7026, label loss: 0.3462, Accuracy: 9450/10000 (94%)\n",
      "epoch: 22\n",
      "Train set: Train loss: 104.9259, label loss: 0.0002, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 99.6021, label loss: 0.3546, Accuracy: 9439/10000 (94%)\n",
      "epoch: 23\n",
      "Train set: Train loss: 106.9548, label loss: 0.0072, Accuracy: 29950/30080 (100%)\n",
      "Valid set: Valid loss: 108.3272, label loss: 0.5143, Accuracy: 9177/10000 (92%)\n",
      "epoch: 24\n",
      "Train set: Train loss: 110.3831, label loss: 0.0140, Accuracy: 29877/30080 (99%)\n",
      "Valid set: Valid loss: 104.0860, label loss: 0.4386, Accuracy: 9279/10000 (93%)\n",
      "epoch: 25\n",
      "Train set: Train loss: 107.7423, label loss: 0.0026, Accuracy: 29976/30080 (100%)\n",
      "Valid set: Valid loss: 102.5467, label loss: 0.3419, Accuracy: 9466/10000 (95%)\n",
      "epoch: 26\n",
      "Train set: Train loss: 106.6343, label loss: 0.0007, Accuracy: 29997/30080 (100%)\n",
      "Valid set: Valid loss: 102.1082, label loss: 0.3296, Accuracy: 9505/10000 (95%)\n",
      "epoch: 27\n",
      "Train set: Train loss: 106.0084, label loss: 0.0002, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 101.3636, label loss: 0.3288, Accuracy: 9518/10000 (95%)\n",
      "epoch: 28\n",
      "Train set: Train loss: 105.5133, label loss: 0.0002, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 100.6424, label loss: 0.3329, Accuracy: 9522/10000 (95%)\n",
      "epoch: 29\n",
      "Train set: Train loss: 105.1279, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 100.3770, label loss: 0.3383, Accuracy: 9522/10000 (95%)\n",
      "epoch: 30\n",
      "Train set: Train loss: 104.7789, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 99.7603, label loss: 0.3429, Accuracy: 9516/10000 (95%)\n",
      "epoch: 31\n",
      "Train set: Train loss: 104.3879, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 99.4436, label loss: 0.3480, Accuracy: 9517/10000 (95%)\n",
      "epoch: 32\n",
      "Train set: Train loss: 104.1149, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.8030, label loss: 0.3523, Accuracy: 9509/10000 (95%)\n",
      "epoch: 33\n",
      "Train set: Train loss: 103.7983, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.7854, label loss: 0.3562, Accuracy: 9510/10000 (95%)\n",
      "epoch: 34\n",
      "Train set: Train loss: 103.5002, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.2382, label loss: 0.3607, Accuracy: 9503/10000 (95%)\n",
      "epoch: 35\n",
      "Train set: Train loss: 103.2127, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.9463, label loss: 0.3652, Accuracy: 9499/10000 (95%)\n",
      "epoch: 36\n",
      "Train set: Train loss: 102.9556, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.4201, label loss: 0.3674, Accuracy: 9491/10000 (95%)\n",
      "epoch: 37\n",
      "Train set: Train loss: 102.6417, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.5361, label loss: 0.3723, Accuracy: 9482/10000 (95%)\n",
      "epoch: 38\n",
      "Train set: Train loss: 102.4476, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.2077, label loss: 0.3755, Accuracy: 9482/10000 (95%)\n",
      "epoch: 39\n",
      "Train set: Train loss: 102.1980, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 96.7477, label loss: 0.3778, Accuracy: 9483/10000 (95%)\n",
      "epoch: 40\n",
      "Train set: Train loss: 103.8140, label loss: 0.0059, Accuracy: 29954/30080 (100%)\n",
      "Valid set: Valid loss: 105.4813, label loss: 0.6210, Accuracy: 9210/10000 (92%)\n",
      "epoch: 41\n",
      "Train set: Train loss: 106.4584, label loss: 0.0095, Accuracy: 29907/30080 (99%)\n",
      "Valid set: Valid loss: 100.7469, label loss: 0.3959, Accuracy: 9450/10000 (94%)\n",
      "epoch: 42\n",
      "Train set: Train loss: 105.0190, label loss: 0.0026, Accuracy: 29976/30080 (100%)\n",
      "Valid set: Valid loss: 99.4695, label loss: 0.3689, Accuracy: 9489/10000 (95%)\n",
      "epoch: 43\n",
      "Train set: Train loss: 104.1593, label loss: 0.0006, Accuracy: 29996/30080 (100%)\n",
      "Valid set: Valid loss: 99.2300, label loss: 0.3780, Accuracy: 9485/10000 (95%)\n",
      "epoch: 44\n",
      "Train set: Train loss: 103.7374, label loss: 0.0002, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 99.1663, label loss: 0.3653, Accuracy: 9510/10000 (95%)\n",
      "epoch: 45\n",
      "Train set: Train loss: 103.3303, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.6997, label loss: 0.3622, Accuracy: 9519/10000 (95%)\n",
      "epoch: 46\n",
      "Train set: Train loss: 103.0520, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.6770, label loss: 0.3647, Accuracy: 9525/10000 (95%)\n",
      "epoch: 47\n",
      "Train set: Train loss: 102.7889, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 98.0974, label loss: 0.3677, Accuracy: 9528/10000 (95%)\n",
      "epoch: 48\n",
      "Train set: Train loss: 102.5752, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.9060, label loss: 0.3712, Accuracy: 9528/10000 (95%)\n",
      "epoch: 49\n",
      "Train set: Train loss: 102.3306, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.6952, label loss: 0.3748, Accuracy: 9525/10000 (95%)\n",
      "epoch: 50\n",
      "Train set: Train loss: 102.0841, label loss: 0.0001, Accuracy: 30000/30080 (100%)\n",
      "Valid set: Valid loss: 97.2277, label loss: 0.3783, Accuracy: 9514/10000 (95%)\n",
      "epoch: 51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fff926448c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mTrain_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_trans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mLamba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLamba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-578acab0084a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_trans, lamba)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#         print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_train_trans = train_trans+semi_trans[:]\n",
    "for epoch in range(50):\n",
    "    print('epoch: {:}'.format(epoch))\n",
    "    if epoch<0:\n",
    "        Train_trans = new_train_trans\n",
    "        Lamba = 0\n",
    "    else:\n",
    "        Train_trans = train_trans\n",
    "        Lamba = 5000\n",
    "    train(epoch, Train_trans, Lamba)\n",
    "    test(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Train set: Train loss: 97.9662, label loss: 0.0474, Accuracy: 75829/77120 (98%)\n",
      "Valid set: Valid loss: 93.0450, label loss: 0.3804, Accuracy: 9514/10000 (95%)\n",
      "epoch: 1\n",
      "Train set: Train loss: 97.1852, label loss: 0.0163, Accuracy: 76516/77120 (99%)\n",
      "Valid set: Valid loss: 91.9468, label loss: 0.3938, Accuracy: 9504/10000 (95%)\n",
      "epoch: 2\n",
      "Train set: Train loss: 96.7133, label loss: 0.0096, Accuracy: 76744/77120 (100%)\n",
      "Valid set: Valid loss: 91.7368, label loss: 0.3917, Accuracy: 9517/10000 (95%)\n",
      "epoch: 3\n",
      "Train set: Train loss: 96.5887, label loss: 0.0078, Accuracy: 76798/77120 (100%)\n",
      "Valid set: Valid loss: 91.6890, label loss: 0.3997, Accuracy: 9511/10000 (95%)\n",
      "epoch: 4\n",
      "Train set: Train loss: 96.6079, label loss: 0.0086, Accuracy: 76781/77120 (100%)\n",
      "Valid set: Valid loss: 91.3914, label loss: 0.4188, Accuracy: 9510/10000 (95%)\n",
      "epoch: 5\n",
      "Train set: Train loss: 96.5050, label loss: 0.0071, Accuracy: 76813/77120 (100%)\n",
      "Valid set: Valid loss: 91.8424, label loss: 0.4333, Accuracy: 9509/10000 (95%)\n",
      "epoch: 6\n",
      "Train set: Train loss: 96.4417, label loss: 0.0048, Accuracy: 76886/77120 (100%)\n",
      "Valid set: Valid loss: 91.0919, label loss: 0.4356, Accuracy: 9504/10000 (95%)\n",
      "epoch: 7\n",
      "Train set: Train loss: 96.4283, label loss: 0.0060, Accuracy: 76854/77120 (100%)\n",
      "Valid set: Valid loss: 91.1320, label loss: 0.4518, Accuracy: 9504/10000 (95%)\n",
      "epoch: 8\n",
      "Train set: Train loss: 96.4087, label loss: 0.0053, Accuracy: 76874/77120 (100%)\n",
      "Valid set: Valid loss: 91.8719, label loss: 0.4843, Accuracy: 9467/10000 (95%)\n",
      "epoch: 9\n",
      "Train set: Train loss: 96.4941, label loss: 0.0049, Accuracy: 76875/77120 (100%)\n",
      "Valid set: Valid loss: 91.2437, label loss: 0.4562, Accuracy: 9505/10000 (95%)\n",
      "epoch: 10\n",
      "Train set: Train loss: 96.4385, label loss: 0.0049, Accuracy: 76879/77120 (100%)\n",
      "Valid set: Valid loss: 91.1382, label loss: 0.4583, Accuracy: 9496/10000 (95%)\n",
      "epoch: 11\n",
      "Train set: Train loss: 96.4510, label loss: 0.0042, Accuracy: 76899/77120 (100%)\n",
      "Valid set: Valid loss: 91.1731, label loss: 0.4772, Accuracy: 9505/10000 (95%)\n",
      "epoch: 12\n",
      "Train set: Train loss: 96.3304, label loss: 0.0034, Accuracy: 76923/77120 (100%)\n",
      "Valid set: Valid loss: 91.0125, label loss: 0.4707, Accuracy: 9520/10000 (95%)\n",
      "epoch: 13\n",
      "Train set: Train loss: 96.2933, label loss: 0.0057, Accuracy: 76856/77120 (100%)\n",
      "Valid set: Valid loss: 90.9042, label loss: 0.4956, Accuracy: 9499/10000 (95%)\n",
      "epoch: 14\n",
      "Train set: Train loss: 96.3612, label loss: 0.0038, Accuracy: 76899/77120 (100%)\n",
      "Valid set: Valid loss: 91.4979, label loss: 0.5083, Accuracy: 9480/10000 (95%)\n",
      "epoch: 15\n",
      "Train set: Train loss: 96.2816, label loss: 0.0039, Accuracy: 76901/77120 (100%)\n",
      "Valid set: Valid loss: 91.0884, label loss: 0.5121, Accuracy: 9504/10000 (95%)\n",
      "epoch: 16\n",
      "Train set: Train loss: 96.2681, label loss: 0.0039, Accuracy: 76906/77120 (100%)\n",
      "Valid set: Valid loss: 91.9215, label loss: 0.5223, Accuracy: 9498/10000 (95%)\n",
      "epoch: 17\n",
      "Train set: Train loss: 96.2490, label loss: 0.0030, Accuracy: 76934/77120 (100%)\n",
      "Valid set: Valid loss: 90.4030, label loss: 0.5343, Accuracy: 9500/10000 (95%)\n",
      "epoch: 18\n",
      "Train set: Train loss: 96.1265, label loss: 0.0041, Accuracy: 76891/77120 (100%)\n",
      "Valid set: Valid loss: 91.1687, label loss: 0.5206, Accuracy: 9499/10000 (95%)\n",
      "epoch: 19\n",
      "Train set: Train loss: 95.9676, label loss: 0.0020, Accuracy: 76962/77120 (100%)\n",
      "Valid set: Valid loss: 91.1391, label loss: 0.5338, Accuracy: 9511/10000 (95%)\n",
      "epoch: 20\n",
      "Train set: Train loss: 96.0974, label loss: 0.0043, Accuracy: 76889/77120 (100%)\n",
      "Valid set: Valid loss: 90.6559, label loss: 0.5286, Accuracy: 9513/10000 (95%)\n",
      "epoch: 21\n",
      "Train set: Train loss: 96.1387, label loss: 0.0036, Accuracy: 76905/77120 (100%)\n",
      "Valid set: Valid loss: 91.0714, label loss: 0.5450, Accuracy: 9510/10000 (95%)\n",
      "epoch: 22\n",
      "Train set: Train loss: 95.9761, label loss: 0.0021, Accuracy: 76949/77120 (100%)\n",
      "Valid set: Valid loss: 91.4272, label loss: 0.5716, Accuracy: 9497/10000 (95%)\n",
      "epoch: 23\n",
      "Train set: Train loss: 96.1557, label loss: 0.0037, Accuracy: 76911/77120 (100%)\n",
      "Valid set: Valid loss: 91.2305, label loss: 0.5601, Accuracy: 9502/10000 (95%)\n",
      "epoch: 24\n",
      "Train set: Train loss: 96.0490, label loss: 0.0023, Accuracy: 76945/77120 (100%)\n",
      "Valid set: Valid loss: 90.8993, label loss: 0.5774, Accuracy: 9503/10000 (95%)\n",
      "epoch: 25\n",
      "Train set: Train loss: 95.6711, label loss: 0.0009, Accuracy: 76986/77120 (100%)\n",
      "Valid set: Valid loss: 90.6011, label loss: 0.5881, Accuracy: 9513/10000 (95%)\n",
      "epoch: 26\n",
      "Train set: Train loss: 95.8759, label loss: 0.0050, Accuracy: 76867/77120 (100%)\n",
      "Valid set: Valid loss: 91.2870, label loss: 0.6265, Accuracy: 9490/10000 (95%)\n",
      "epoch: 27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cd69f5a6dde5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: {:}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_train_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamba\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-578acab0084a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_trans, lamba)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#         print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/anaconda/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "semi()\n",
    "new_train_trans = train_trans+semi_trans[:]\n",
    "for epoch in range(40):\n",
    "    print('epoch: {:}'.format(epoch))\n",
    "    train(epoch, new_train_trans, lamba=5000)\n",
    "    test(epoch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(testset, open(\"test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_true = test_loader.dataset.test_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_array = label_true - label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.where(diff_array != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "true_label = pd.DataFrame(label_true, columns=['label'])\n",
    "true_label.reset_index(inplace=True)\n",
    "true_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('sample_submission.csv', index=False)\n",
    "true_label.to_csv('true_label.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
